{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid19 - Google Cloud Platform Data Pipeline\n",
    "The helper code, functions etc. that were used to create the ETL Data Pipeline for the project. Well commented tutorial. Can be used to follow along and the functions can be used for making modifications to the pipeline.\n",
    "<div>\n",
    "    <img src=\"Covid_19_Data_Pipeline.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from psycopg2 import ProgrammingError, errors, IntegrityError\n",
    "import subprocess\n",
    "\n",
    "# Covid19.org API Data ingestion helpers :\n",
    "from Covid19_india_org_api import make_dataframe, get_test_dataframe, make_state_dataframe\n",
    "\n",
    "# Cloud SQL Server Credentials\n",
    "db_user = 'postgres'\n",
    "db_password = '***REMOVED***'\n",
    "cloud_sql_ip = 'localhost:5432'# connecting through CloudSQL proxy\n",
    "db_name = 'covid19-india'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections :\n",
    "[1.](#CN) Connecting to CloudSQL PostgreSQL Instance.\n",
    "\n",
    "[2.](#MOD) Creating and Modifying Tables \n",
    "\n",
    "[3.](#ADD) Adding Records from cleaned CSVs/Pandas DataFrames to Tables using Pandas\n",
    "\n",
    "[4.](#BKP) Database Backup\n",
    "\n",
    "[5.](#DVV) Data Validation Before Ingestion\n",
    "\n",
    "[6.](#CFN) Cloud Functions need to be modified if the Data Pipeline is modified. <br> Notebook versions of :\n",
    "1. Google cloud function to fetch raw data - GCP_Cloud_function.py <br> \n",
    "2. Google cloud function to load Raw data, transform it and move to CloudSQL Server - Cloud_function_Ingestion_SQL.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"CN\"></a>Connecting to Google Cloud SQL Server\n",
    "1. Connect to CloudSQL instance using CloudSQL proxy and SQL Alchemy engine:\n",
    "   * For GCP accounts with permission - Cloud SQL -> Client, Admin etc. \n",
    "   * cloud_sql_proxy needs to be running and set-up on your local machine and your credentials need to accessible.\n",
    "   * Lets the User update, modify the tables acc. to the permission given.\n",
    "   * Receive CloudSQL Instance Public IP and your username, password.\n",
    "   * Navigate to CloudSQL \n",
    "   * Use SQL Alchemy to connect.\n",
    "   \n",
    "Refer : https://cloud.google.com/sql/docs/postgres/connect-external-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create SQLAlchemy Engine and connect to CloudSQl instance through CloudSQL proxy\n",
    "\n",
    "# Credentials\n",
    "db_user = 'postgres'\n",
    "db_password = '***REMOVED***'\n",
    "cloud_sql_ip = 'localhost:5432'# connecting through proxy\n",
    "db_name = 'covid19-india'\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{cloud_sql_ip}/{db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['overall_stats', 'states_info', 'testing_stats']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List available tables \n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"MOD\"></a>Creating and Modifying SQL Database using SQLAlchemy\n",
    "SQLAlchemy provides an easy to use engine to interface with your RDBMS. Pass queries to a connected Database Using it.\n",
    "\n",
    "Refer : https://docs.sqlalchemy.org/en/13/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Helper Functions - Creating Tables\n",
    "After Connecting SQLAlchemy to a Database, you can directly create tables using the engine.<br> The tables here are created using the current data dictionary and schema :\n",
    "<div>\n",
    "<img src=\"DB_ERD.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_table_overall_stats(engine):\n",
    "    \"\"\" Initial setup of overall_stats table according to Schema\n",
    "    (rigid, hard-coded, can cause problems) - consult others. \n",
    "    \"\"\"\n",
    "    # Creating Overall_stats table\n",
    "    engine.execute(\"\"\" CREATE TABLE overall_stats(\n",
    "                \"Date\" DATE PRIMARY KEY,\n",
    "                \"DailyConfirmed\" INT NOT NULL,\n",
    "                \"DailyDeceased\" INT NOT NULL,\n",
    "                \"DailyRecovered\" INT NOT NULL,\n",
    "                \"TotalConfirmed\" INT NOT NULL,\n",
    "                \"TotalDeceased\" INT NOT NULL,\n",
    "                \"TotalRecovered\" INT NOT NULL\n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_table_testing_stats(engine):\n",
    "    \"\"\" Initial setup of testing_stats table.\n",
    "    \"\"\"\n",
    "# Creating testing stats table\n",
    "    engine.execute(\"\"\" CREATE TABLE testing_stats(\n",
    "                \"Date\" DATE PRIMARY KEY,\n",
    "                \"TestingSamples\" INT NOT NULL,\n",
    "                FOREIGN KEY(\"Date\")\n",
    "                    REFERENCES overall_stats(\"Date\")\n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_table_state_info(engine):\n",
    "    \"\"\" Initial setup of state_info table, used pandas.io.sql.get_schema to create schema and added\n",
    "    keys later due to the number of columns. \n",
    "    \"\"\"\n",
    "    # Creating state_info table\n",
    "    engine.execute(\"\"\"CREATE TABLE \"states_info\" (\n",
    "    \"Date\" DATE ,\n",
    "    \"State\" TEXT,\n",
    "    \"Confirmed\" INTEGER,\n",
    "    \"Deceased\" INTEGER,\n",
    "    \"Recovered\" INTEGER,\n",
    "    PRIMARY KEY(\"Date\", \"State\"),\n",
    "    FOREIGN KEY(\"Date\")\n",
    "    REFERENCES overall_stats(\"Date\")\n",
    "    )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make sure engine is connected to SQL Server.\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{cloud_sql_ip}/{db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating Tables \n",
    "\n",
    "create_table_overall_stats(engine)\n",
    "create_table_testing_stats(engine)\n",
    "create_table_state_info(engine)\n",
    "\n",
    "# List available tables \n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"ADD\"></a>Adding Data to SQL Server : Data Ingestion Functions\n",
    "Uses the Covid19.org API Data ingsestion functions that provide the cleaned Dataframes acc. to schema.<br>\n",
    "The Ingestion function requires just a Clean Pandas DF that matches the schema of the table in which data is to sent.<br>\n",
    "Uses DF.to_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# append problem : duplicate key values raises error even for append. Shouldn't happen with append but here we are.\n",
    "# workaround. fetch length of existing records in table and then only store records after that. Can be problematic.\n",
    "# Cannot replace due to the presence of foreign key.\n",
    "\n",
    "def add_data_table(engine, tablename, df):\n",
    "    \"\"\" Appends New Data to table if it exists\n",
    "    Takes in engine connected to DB, tablename and dataframe to store.\n",
    "    Throws error if 1. Table Doesn't Exist, 2. incorrect table and dataframe ?(abstract this coice away from user)\n",
    "    Problematic for testing_table as it has duplicates. - Possible solution, find last index and not length.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        results = engine.execute(f\"\"\"SELECT * FROM {tablename}\"\"\")\n",
    "        num_records = len(results.fetchall())\n",
    "        print(f'{num_records} Records in {tablename}')\n",
    "\n",
    "        df[num_records:].to_sql(tablename, engine, if_exists='append')\n",
    "        print(f'Added {len(df[num_records:])} Records to table')\n",
    "    \n",
    "    # Catches all error, currently no logging/fault tolerance.\n",
    "    except IntegrityError as e:\n",
    "        print(e)\n",
    "        if err == IntegrityError :\n",
    "            print('Update Master Table first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make sure engine is connected to SQL Server.\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{cloud_sql_ip}/{db_name}')\n",
    "\n",
    "# List available tables \n",
    "engine.table_names()\n",
    "\n",
    "# Check current Data in Tables \n",
    "\n",
    "#pd.read_sql('SELECT * FROM states_info', engine , parse_dates = True, index_col ='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 Records in overall_stats\n",
      "Added 4 Records to table\n"
     ]
    }
   ],
   "source": [
    "# Add Data to the tables\n",
    "\n",
    "add_data_table(engine, 'overall_stats', make_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 Records in testing_stats\n",
      "Added 6 Records to table\n"
     ]
    }
   ],
   "source": [
    "# Test data has duplicate entries, fix that.\n",
    "test = get_test_dataframe()\n",
    "test = test.loc[~test.index.duplicated(keep='last')]\n",
    "add_data_table(engine, 'testing_stats', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5889 Records in states_info\n",
      "Added 156 Records to table\n"
     ]
    }
   ],
   "source": [
    "add_data_table(engine, 'states_info', make_state_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"BKP\"></a> Dumping PostgreSQL DB\n",
    "Backed up using GUI for now. <br>\n",
    "https://www.postgresqltutorial.com/postgresql-backup-database/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def backup_dB(path):\n",
    "    subprocess.run(['pg_dump', '--host=localhost', '--dbname=Covid19-India',\n",
    "                    '--username=postgres', '--no-password','--format=p',\n",
    "                    f'--file={path}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "backup_dB('Covid19-India_backup.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Main Function - Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Ingesting overall stats data \n",
    "data = make_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 Records in overall_stats\n",
      "Added 2 Records to table\n"
     ]
    }
   ],
   "source": [
    "add_data_table(engine, 'overall_stats', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 Records in testing_stats\n",
      "Added 1 Records to table\n"
     ]
    }
   ],
   "source": [
    "# Ingesting Testing Data \n",
    "\n",
    "# test has duplicates for a single date, will fail the unique constraint for key, remove first.\n",
    "test = get_test_dataframe()\n",
    "test = test.loc[~test.index.duplicated(keep='last')]\n",
    "add_data_table(engine, 'testing_stats', test[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6279 Records in states_info\n",
      "Added 78 Records to table\n"
     ]
    }
   ],
   "source": [
    "# Ingesting State column\n",
    "\n",
    "states = make_state_dataframe()\n",
    "\n",
    "add_data_table(engine, 'states_info', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"DVV\"></a> Data Validation Before Ingestion to SQL Server\n",
    "Using CSVValidator(outdated project, not that useful), why use this in place of assertion tests(that would only check data type for a python object and not the data in the csv, like range etc.) and unit tests(because we're not testing code, outputs etc., but data) <br> <br>\n",
    "PandasSchema - Another Open Source project that allows schema validation, Unique constraint, Date format , List validation<br> Idea is to have a schema dataframe stored as backup that can be used to check the incoming dataframes. Not Deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pandas_schema import Column, Schema\n",
    "import numpy as np\n",
    "from pandas_schema.validation import InListValidation, IsDtypeValidation, IsDistinctValidation, DateFormatValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main_data = make_dataframe().reset_index()\n",
    "state_data = make_state_dataframe().reset_index()\n",
    "testing_data = get_test_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# also validate no null data and that final date is today. \n",
    "def test_missing(df):\n",
    "    missing_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() != 0 :\n",
    "                missing_columns.append(col)  \n",
    "    if len(missing_columns) > 0:\n",
    "        print(f'Missing values in DF ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Validate schema \n",
    "\n",
    "schema_main = Schema([\n",
    "    Column('Date', [DateFormatValidation('%Y-%m-%d'), IsDistinctValidation(), IsDtypeValidation(np.dtype('<M8[ns]'))]),\n",
    "    Column('DailyConfirmed',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('DailyDeceased',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('DailyRecovered',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('TotalConfirmed',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('TotalDeceased',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('TotalRecovered',[IsDtypeValidation(np.dtype('int64'))])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "errors_main = schema_main.validate(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for error in errors_main:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Validate schema \n",
    "\n",
    "schema_state = Schema([\n",
    "    Column('Date', [DateFormatValidation('%Y-%m-%d'), IsDtypeValidation(np.dtype('<M8[ns]'))]),\n",
    "    Column('State',[IsDtypeValidation(np.dtype(np.dtype('O'))), InListValidation(['Andaman and Nicobar Islands', 'Andhra Pradesh',\n",
    "       'Arunachal Pradesh', 'Assam', 'Bihar', 'Chandigarh',\n",
    "       'Chhattisgarh', 'DD', 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "       'Delhi', 'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh',\n",
    "       'Jammu and Kashmir', 'Jharkhand', 'Karnataka', 'Kerala', 'Ladakh',\n",
    "       'Lakshadweep', 'Madhya Pradesh', 'Maharashtra', 'Manipur',\n",
    "       'Meghalaya', 'Mizoram', 'Nagaland', 'Odisha', 'Puducherry',\n",
    "       'Punjab', 'Rajasthan', 'Sikkim', 'State Unassigned', 'Tamil Nadu',\n",
    "       'Telangana', 'Total', 'Tripura', 'Uttar Pradesh', 'Uttarakhand',\n",
    "       'West Bengal'])]),\n",
    "    Column('Confirmed',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('Deceased',[IsDtypeValidation(np.dtype('int64'))]),\n",
    "    Column('Recovered',[IsDtypeValidation(np.dtype('int64'))])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "errors_state = schema_state.validate(state_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for error in errors_state:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_missing(state_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TestingSamples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>13125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  TestingSamples\n",
       "0 2020-03-13            6500\n",
       "1 2020-03-18           13125"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Validate schema \n",
    "\n",
    "schema_test = Schema([\n",
    "    Column('Date', [DateFormatValidation('%Y-%m-%d'), IsDtypeValidation(np.dtype('<M8[ns]'))]),\n",
    "    Column('TestingSamples',[IsDtypeValidation(np.dtype('int64'))])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "errors_test = schema_test.validate(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for error in errors_test:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_missing(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# now to check if the final data of update of the Dfs is the same or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_state = main_data.Date[-1:].dt.to_pydatetime() == state_data.Date[-1:].dt.to_pydatetime()\n",
    "main_test = main_data.Date[-1:].dt.to_pydatetime() == testing_data.Date[-1:].dt.to_pydatetime()\n",
    "\n",
    "main_state == main_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"CFN\"></a> Cloud Functions\n",
    "These might not be consistent with the scripts. in the Data_Pipeline Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Data Ingestion (Raw- Bucket) to (Clean - Cloud SQL) Cloud Function\n",
    "1. Create connection to bucket. \n",
    "2. Download files from Bucket. \n",
    "3. (optional) Data validation of CSVs.- CSV validator (Validate when before Ingestion to Clean- Skipped Right now)\n",
    "4. Use Pandas as an intermediary to clean data. \n",
    "5. Create SQLAlchemy connection to CloudSQL Server.\n",
    "6. Upload Pandas DF to Cloud SQL Using SQLAlchmey.\n",
    "7. For completely empty tables and ingesting entire data to date (overall - 190, state - 5694, testing - 135 records):<br>Function execution took 22190 ms, finished with status code: 200\n",
    "8. For fetching a single day's data :<br> A. Fetch Raw Data Using fetch_raw_covid_api_data : \"Function execution took 6836 ms, finished with status code: 200\"<br>\n",
    "B. Cleaning Data and Ingesting into CloudSQl : \"Function execution took 2147 ms, finished with status code: 200\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import pg8000 # databse driver\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def download_folder_bucket(bucket, bucket_folder, local_folder):\n",
    "    \"\"\"Download all files from a GCS bucket folder to a local folder.\n",
    "    \"\"\"\n",
    "    # list of filenames in bucket_folder\n",
    "    file_list = [file.name for file in bucket.list_blobs(prefix=bucket_folder)]\n",
    "    \n",
    "    # iterate over blobs and doenload to local folder + filename\n",
    "\n",
    "    for file in file_list :\n",
    "        blob = bucket.blob(file)\n",
    "        # filename by splitting name by '/' and keeping last item\n",
    "        filename = blob.name.split('/')[-1]\n",
    "        # download to local folder\n",
    "        blob.download_to_filename(local_folder + filename)\n",
    "    return f'Downloaded {len(file_list)} Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_data_table(engine, tablename, df):\n",
    "    \"\"\" Appends New Data to table if it exists\n",
    "    Takes in engine connected to DB, tablename and dataframe to store.\n",
    "    Throws error if 1. Table Doesn't Exist, 2. incorrect table and dataframe ?(abstract this coice away from user)\n",
    "    Problematic for testing_table as it has duplicates. - Possible solution, find last index and not length.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        results = engine.execute(f\"\"\"SELECT * FROM {tablename}\"\"\")\n",
    "        num_records = len(results.fetchall())\n",
    "        print(f'{num_records} Records in {tablename}')\n",
    "\n",
    "        df[num_records:].to_sql(tablename, engine, if_exists='append')\n",
    "        print(f'Added {len(df[num_records:])} Records to table')\n",
    "    \n",
    "    # Just can't seem to get errors to work \n",
    "    except :\n",
    "        print('Errored. Investigate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create SQLAlchemy connection to CloudSQL Server.\n",
    "\n",
    "# Remember - storing secrets in plaintext is potentially unsafe.\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\" Connects to Cloud SQL DB Using provided Unix Socket. Username, Password etc. Hardcoded.\n",
    "    Problematic.\n",
    "    \"\"\"\n",
    "    db_user = 'postgres'\n",
    "    db_pass = ''\n",
    "    db_name = 'covid19-data'\n",
    "    db_socket_dir = os.environ.get(\"DB_SOCKET_DIR\", \"/cloudsql\")\n",
    "    cloud_sql_connection_name = 'covid19-india-analysis-284814:***REMOVED***:covid19-data-server'\n",
    "\n",
    "    engine = sqlalchemy.create_engine(\n",
    "        # Equivalent URL:\n",
    "        # postgres+pg8000://<db_user>:<db_pass>@/<db_name>\n",
    "        #                         ?unix_sock=<socket_path>/<cloud_sql_instance_name>/.s.PGSQL.5432\n",
    "        sqlalchemy.engine.url.URL(\n",
    "            drivername=\"postgres+pg8000\",\n",
    "            username=db_user,  # e.g. \"my-database-user\"\n",
    "            password=db_pass,  # e.g. \"my-database-password\"\n",
    "            database=db_name,  # e.g. \"my-database-name\"\n",
    "            query={\n",
    "                \"unix_sock\": \"{}/{}/.s.PGSQL.5432\".format(\n",
    "                    db_socket_dir,  # e.g. \"/cloudsql\"\n",
    "                    cloud_sql_connection_name)  # i.e \"<PROJECT-NAME>:<INSTANCE-REGION>:<INSTANCE-NAME>\"\n",
    "            }\n",
    "        ),\n",
    "        # ... Specify additional properties here.\n",
    "    )\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def main(request):\n",
    "    \"\"\" Driver function for CLoud Function. Request doesn't do anything. \n",
    "    \"\"\"\n",
    "    # Create GCS client \n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # connect to a bucket \n",
    "    bucket = storage_client.get_bucket('covid19-india-analysis-bucket')\n",
    "    \n",
    "    # Download RAW CSVs from GCS Bucket to Cloud Function temp. storage.\n",
    "    download_folder_bucket(bucket, 'Data/Raw/', '/tmp/')\n",
    "    \n",
    "    # Loading and Transforming data\n",
    "    data = pd.read_csv('/tmp/COVID_India_National.csv', parse_dates=True, index_col=0)\n",
    "    state = pd.read_csv('/tmp/COVID_India_State.csv', parse_dates=True, index_col=0)\n",
    "    # Load and clean test data \n",
    "    test = pd.read_csv('/tmp/COVID_India_Test_data.csv', parse_dates=True, index_col=0)\n",
    "    test = test.loc[~test.index.duplicated(keep='last')]\n",
    "    \n",
    "    # Connect to CloudSQL DB\n",
    "    engine = connect_db()\n",
    "    \n",
    "    # Uploading Data to DB \n",
    "    add_data_table(engine, 'overall_stats', data)\n",
    "    add_data_table(engine, 'states_info', state)\n",
    "    add_data_table(engine, 'testing_stats', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Misc.\n",
    "Code used to modify the PD stateDF structure for Wide DB structure. Important PD multindex, Pivot table, Melt Examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Data_Ingestion.py',\n",
       " 'Data_Ingestion.ipynb',\n",
       " '__pycache__',\n",
       " 'Covid19_india_org_api.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'Cloud_function_Ingestion_SQL.py',\n",
       " 'tmp',\n",
       " 'GCP_Cloud_function.py']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "states = make_state_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Melt stacked Column index Dataframe to long format\n",
    "states.reset_index(inplace=True)\n",
    "new_data = states.melt(id_vars=['Date'])\n",
    "# renaming orphan column with state data\n",
    "new_data.rename(axis = 1, mapper={None: 'State'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pivoting to reshape Status column values(Recovered, Confirmed, Deceased Cases) to columns \n",
    "pivot_data = new_data.pivot_table(index = ['Date','State'],columns = 'Status', values = 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reset index to transfer stacked index Date for to date column\n",
    "final_data = pivot_data.reset_index().set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Just a series form of the above DF \n",
    "#new_data.groupby(['Date', 'State', 'Status'])['value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creates the Stacked dataframe Again \n",
    "#new_data.pivot_table(values='value', index = 'Date', columns=['State', 'Status'])"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Data_ingestion.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
